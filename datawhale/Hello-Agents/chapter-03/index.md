# 第 3 章 大语言模型基础

现代智能体是如何工作的？

## 3.1 语言模型与Transformer架构

### 3.1.1 从N-gram到RNN

**语言模型 (Language Model, LM)** 是自然语言处理的核心，其根本任务是计算一个词序列（即一个句子）出现的概率。一个好的语言模型能够告诉我们什么样的句子是通顺的、自然的。在多智能体系统中，语言模型是智能体理解人类指令、生成回应的基础。

#### (1) 统计语言模型与N-gram的思想

在深度学习兴起之前，统计方法是语言模型的主流。其核心思想是，一个句子出现的概率，等于该句子中每个词出现的条件概率的连乘。其概率表示为：

$$
P(S) = P(w_1, w_2, ..., w_m) = P(w_1) \cdot P(w_2 | w_1)\cdot P(w_3 | w_1, w_2) ... P(w_m | w_1, ..., w_{m-1})
$$

这个公式被称为**概率的链式法则**。然而，直接计算这个公式几乎是不可能的，因为像 $P(w_m | w_1, ..., w_{m-1})$ 这样的条件概率太难从语料库中估计了，词序列$w_1, ..., w_{m-1}$ 可能从未在训练数据中出现过。

为了解决这个问题，研究者引入了**马尔可夫假设 (Markov Assumption)** 。其核心思想是：我们不必回溯一个词的全部历史，可以近似地认为，一个词的出现概率只与它前面有限的 $n−1$ 个词有关。基于这个假设建立的语言模型，我们称之为 **N-gram模型**。这里的 "N" 代表我们考虑的上下文窗口大小。让我们来看几个最常见的例子来理解这个概念：

这些概率可以通过在大型语料库中进行**最大似然估计(Maximum Likelihood Estimation,MLE)** 来计算。这个术语听起来很复杂，但其思想非常直观：最可能出现的，就是我们在数据中看到次数最多的。例如，对于 Bigram 模型，我们想计算在词 $w_{i-1}$出现后，下一个词是 $w_i$的概率$P(w_i | w_{i-1})$。根据最大似然估计，这个概率可以通过简单的技术来估算：

$$
P(w_i | w_{i-1})=\frac{Count(w_{i-1}, w_i)}{Count(w_{i-1})}
$$

这里的 `Count()` 函数就代表“计数”：
- $Count(w_{i-1}, w_i)$: 表示词对 $(w_{i-1}, wi)$在语料中连续出现的总次数。
- $Count(w_{i-1})$: 表示单个词 $w_{i-1}$ 在语料库中出现的总次数。

公式的含义就是：我们用“词对 ${Count(w_{i-1}, w_i)}$ 出现的次数”除以“词 $Count(w_{i-1})$ 出现的总次数”，来作为 $P(w_{i-1} | P(w_i))$ 的一个近似估计。

N-gram模型虽然简单，但是有两个致命缺陷：
1. 数据稀疏性：如果一个词序列从未在语料库中出现，其概率估计就为 0，这显然是不合理的。
2. 泛化能力差：模型无法理解词与词之间的语义相似性。


#### (2) 神经网络语言模型与词嵌入

N-gram 模型的根本缺陷在于它将词视为孤立、离散的符号。为了克服这个问题，研究者们转向了神经网络，并提出了一种思想：用连续的向量来表示词。

其核心思想可以分为两步：
1. 构建一个语义空间：创建一个高维的连续向量空间，然后将词汇表中的每个词都映射为该空间中的一个点。这个点（即向量）就被称为**词嵌入 (Word Embedding)** 或词向量。在这个空间里，语义上相近的词，它们对应的向量在空间中的位置也相近。例如，`agent` 和 `robot` 的向量会靠得很近，而 `agent` 和 `apple` 的向量会离得很远。
2. 学习从上下文到下一个词的映射：利用神经网络的强大拟合能力，来学习一个函数。这个函数的输入是前 `n−1` 个词的词向量，输出是词汇表中每个词在当前上下文后出现的概率分布。

模型为了完成“预测下一个词”这个任务，会不断调整每个词的向量位置，最终使这些向量能够蕴含丰富的语义信息。一旦我们将词转换成了向量，我们就可以用数学工具来度量它们之间的关系。最常用的方法是余弦相似度 (Cosine Similarity) ，它通过计算两个向量夹角的余弦值来衡量它们的相似性。

$$
similarity(a, b) = cos(θ) = \frac{a \cdot b}{|a|*|b|}
$$

这个公式的含义是：
- 如果两个向量方向完全相同，夹角为0°，余弦值为1，表示完全相关。
- 如果两个向量方向正交，夹角为90°，余弦值为0，表示毫无关系。
- 如果两个向量方向完全相反，夹角为180°，余弦值为-1，表示完全负相关。

通过这种方式，词向量不仅能捕捉到“同义词”这类简单的关系，还能捕捉到更复杂的类比关系。

一个著名的例子展示了词向量捕获到的语义关系：`vector('King') - vector('Man') + vector('Woman')`这个向量运算的结果，在向量空间中与 `vector('Queen')` 的位置惊人地接近。这好比在进行语义的平移：我们从“国王”这个点出发，减去“男性”的向量，再加上“女性”的向量，最终就抵达了“女王”的位置。这证明了词嵌入能够学习到“性别”、“皇室”这类抽象概念。


``` py

```

神经网络语言模型通过词嵌入，成功解决了 N-gram 模型的泛化能力差的问题。然而，它仍然有一个类似 N-gram 的限制：上下文窗口是固定的。它只能考虑固定数量的前文，这为能处理任意长序列的循环神经网络埋下了伏笔。